{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Primary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convBlock(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(convBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channel, output_channel, 3)\n",
    "        self.conv2 = nn.Conv2d(output_channel, output_channel, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input:torch.tensor):\n",
    "        input = self.conv1(input)\n",
    "        input = self.relu(input)\n",
    "        input = self.conv2(input)\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetEncoder(nn.Module):\n",
    "    def __init__(self, channel_list:list):\n",
    "        super(UnetEncoder, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.block_list = []\n",
    "\n",
    "        for i in range(len(channel_list)-1):\n",
    "            self.block_list.append(convBlock(channel_list[i], channel_list[i+1]))\n",
    "    \n",
    "    def forward(self, input:torch.tensor):\n",
    "        layered_encoder_out = []\n",
    "\n",
    "        for block in self.block_list:\n",
    "            input = block(input)\n",
    "            layered_encoder_out .append(input)\n",
    "            input = self.pool(input)\n",
    "\n",
    "        return layered_encoder_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetDecoder(nn.Module):\n",
    "    def __init__(self, channel_list:list):\n",
    "        super(UnetDecoder, self).__init__()\n",
    "        self.channel_list = channel_list\n",
    "        self.block_list = []\n",
    "        self.up_sampler = []\n",
    "\n",
    "        for i in range(len(channel_list)-1):\n",
    "            self.up_sampler.append(nn.ConvTranspose2d(channel_list[i], channel_list[i+1], 2, 2))\n",
    "\n",
    "        for i in range(len(channel_list)-1):\n",
    "            self.block_list.append(convBlock(channel_list[i], channel_list[i+1]))\n",
    "\n",
    "    # layer concat takes from the encoder layered ouptut for concat\n",
    "    def forward(self, input, layered_concat):\n",
    "        for i in range(len(self.channel_list)-1):\n",
    "            input = self.up_sampler[i](input)\n",
    "            concat_feature = self.crop(layered_concat[i], input)\n",
    "            input = torch.concat([input, concat_feature], dim=1)\n",
    "            input = self.block_list[i](input)\n",
    "        return input\n",
    "\n",
    "    def crop(self, concat_feature, input):\n",
    "            B, C, H, W = input.shape\n",
    "            concat_feature = torchvision.transforms.CenterCrop([H, W])(concat_feature)\n",
    "            return concat_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channel_list:list, out_channel_list:list, classes=1, keep_dim=True, output_size=(1024, 1024)):\n",
    "        super(UNET, self).__init__()\n",
    "        self.encoder = UnetEncoder(in_channel_list)\n",
    "        self.decoder = UnetDecoder(out_channel_list)\n",
    "        self.compressor = nn.Conv2d(out_channel_list[-1], classes, 1)\n",
    "        self.output_size = output_size\n",
    "        self.keep_dim = keep_dim\n",
    "    \n",
    "    def forward(self, input):\n",
    "        encoder_output = self.encoder(input)\n",
    "        encoder_output = list(reversed(encoder_output))\n",
    "        output = self.decoder(encoder_output[0], encoder_output[1:])\n",
    "        output = self.compressor(output)\n",
    "        if self.keep_dim:\n",
    "            output = F.interpolate(output, self.output_size)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 568, 568])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc_block = convBlock(1, 64)\n",
    "x = torch.randn(1, 1, 572, 572)\n",
    "x = enc_block(x)\n",
    "display(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 568, 568])\n",
      "torch.Size([1, 128, 280, 280])\n",
      "torch.Size([1, 256, 136, 136])\n",
      "torch.Size([1, 512, 64, 64])\n",
      "torch.Size([1, 1024, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "chan_list = [3,64,128,256,512,1024]\n",
    "encoder = UnetEncoder(channel_list=chan_list)\n",
    "# input image\n",
    "x    = torch.randn(1, 3, 572, 572)\n",
    "ftrs = encoder(x)\n",
    "for ftr in ftrs: print(ftr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 388, 388])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_list = [1024, 512, 256, 128, 64]\n",
    "decoder = UnetDecoder(channel_list=channel_list)\n",
    "x = torch.randn(1, 1024, 28, 28)\n",
    "decoder(x, ftrs[::-1][1:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024, 1024])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_chan = [3,64,128,256,512,1024]\n",
    "out_chan = [1024, 512, 256, 128, 64]\n",
    "unet = UNET(in_chan, out_chan, keep_dim=True)\n",
    "x    = torch.randn(1, 3, 572, 572)\n",
    "unet(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5d6c2be6262c78dadbf075e7ed7eef3fc1090c1044fdcff36d11e01ba01e819"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
