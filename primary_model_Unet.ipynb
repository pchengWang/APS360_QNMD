{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from IPython.display import display\n",
    "\n",
    "import pydicom\n",
    "import glob\n",
    "\n",
    "import time\n",
    "import datetime \n",
    "from tqdm import tqdm \n",
    "\n",
    "from copy import deepcopy\n",
    "# from torchkeras.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to transform rle to mask img\n",
    "from mask_functions import rle2mask\n",
    "from skimage.color import label2rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Primary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convBlock(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(convBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channel, output_channel, 3)\n",
    "        self.conv2 = nn.Conv2d(output_channel, output_channel, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input:torch.tensor):\n",
    "        input = self.conv1(input)\n",
    "        input = self.relu(input)\n",
    "        input = self.conv2(input)\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetEncoder(nn.Module):\n",
    "    def __init__(self, channel_list:list):\n",
    "        super(UnetEncoder, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.block_list = []\n",
    "\n",
    "        for i in range(len(channel_list)-1):\n",
    "            self.block_list.append(convBlock(channel_list[i], channel_list[i+1]))\n",
    "    \n",
    "    def forward(self, input:torch.tensor):\n",
    "        layered_encoder_out = []\n",
    "\n",
    "        for block in self.block_list:\n",
    "            input = block(input)\n",
    "            layered_encoder_out .append(input)\n",
    "            input = self.pool(input)\n",
    "\n",
    "        return layered_encoder_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetDecoder(nn.Module):\n",
    "    def __init__(self, channel_list:list):\n",
    "        super(UnetDecoder, self).__init__()\n",
    "        self.channel_list = channel_list\n",
    "        self.block_list = []\n",
    "        self.up_sampler = []\n",
    "\n",
    "        for i in range(len(channel_list)-1):\n",
    "            self.up_sampler.append(nn.ConvTranspose2d(channel_list[i], channel_list[i+1], 2, 2))\n",
    "\n",
    "        for i in range(len(channel_list)-1):\n",
    "            self.block_list.append(convBlock(channel_list[i], channel_list[i+1]))\n",
    "\n",
    "    # layer concat takes from the encoder layered ouptut for concat\n",
    "    def forward(self, input, layered_concat):\n",
    "        for i in range(len(self.channel_list)-1):\n",
    "            input = self.up_sampler[i](input)\n",
    "            concat_feature = self.crop(layered_concat[i], input)\n",
    "            input = torch.concat([input, concat_feature], dim=1)\n",
    "            input = self.block_list[i](input)\n",
    "        return input\n",
    "\n",
    "    def crop(self, concat_feature, input):\n",
    "            B, C, H, W = input.shape\n",
    "            concat_feature = torchvision.transforms.CenterCrop([H, W])(concat_feature)\n",
    "            return concat_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channel_list:list, out_channel_list:list, classes=1, keep_dim=True, output_size=(1024, 1024)):\n",
    "        super(UNET, self).__init__()\n",
    "        self.encoder = UnetEncoder(in_channel_list)\n",
    "        self.decoder = UnetDecoder(out_channel_list)\n",
    "        self.compressor = nn.Conv2d(out_channel_list[-1], classes, 1)\n",
    "        self.output_size = output_size\n",
    "        self.keep_dim = keep_dim\n",
    "    \n",
    "    def forward(self, input):\n",
    "        encoder_output = self.encoder(input)\n",
    "        encoder_output = list(reversed(encoder_output))\n",
    "        output = self.decoder(encoder_output[0], encoder_output[1:])\n",
    "        output = self.compressor(output)\n",
    "        if self.keep_dim:\n",
    "            output = F.interpolate(output, self.output_size)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 568, 568])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc_block = convBlock(1, 64)\n",
    "x = torch.randn(1, 1, 572, 572)\n",
    "x = enc_block(x)\n",
    "display(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 568, 568])\n",
      "torch.Size([1, 128, 280, 280])\n",
      "torch.Size([1, 256, 136, 136])\n",
      "torch.Size([1, 512, 64, 64])\n",
      "torch.Size([1, 1024, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "chan_list = [3,64,128,256,512,1024]\n",
    "encoder = UnetEncoder(channel_list=chan_list)\n",
    "# input image\n",
    "x    = torch.randn(1, 3, 572, 572)\n",
    "ftrs = encoder(x)\n",
    "for ftr in ftrs: print(ftr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 388, 388])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_list = [1024, 512, 256, 128, 64]\n",
    "decoder = UnetDecoder(channel_list=channel_list)\n",
    "x = torch.randn(1, 1024, 28, 28)\n",
    "decoder(x, ftrs[::-1][1:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_chan = [3,64,128,256,512,1024]\n",
    "out_chan = [1024, 512, 256, 128, 64]\n",
    "unet = UNET(in_chan, out_chan, keep_dim=True)\n",
    "x    = torch.randn(1, 3, 572, 572)\n",
    "unet(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_folder = r'D:\\UT_Third_Year\\UT_TY_Fall\\APS360\\Segmentation_data\\archive'\n",
    "train_data_path = r'D:\\UT_Third_Year\\UT_TY_Fall\\APS360\\Segmentation_data\\archive\\dicom-images-train'\n",
    "train_data_path = Path(train_data_path)\n",
    "work_folder = Path(work_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rle_path = work_folder / 'train-rle.csv'\n",
    "train_rle = pd.read_csv(train_rle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.5597.151787518...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.12515.15178752...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.4904.151787518...</td>\n",
       "      <td>175349 7 1013 12 1009 17 1005 19 1003 20 1002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.32579.15178751...</td>\n",
       "      <td>407576 2 1021 7 1015 10 1013 12 1011 14 1008 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.32579.15178751...</td>\n",
       "      <td>252069 1 1021 3 1020 4 1018 5 1018 6 1016 7 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11577</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.4461.151787518...</td>\n",
       "      <td>592067 6 1016 10 1012 14 1007 18 1004 20 1003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11578</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.4461.151787518...</td>\n",
       "      <td>610576 3 1001 38 981 53 966 63 956 73 947 87 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11579</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.32730.15178751...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11580</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.13252.15178752...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11581</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.12050.15178752...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11582 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ImageId  \\\n",
       "0      1.2.276.0.7230010.3.1.4.8323329.5597.151787518...   \n",
       "1      1.2.276.0.7230010.3.1.4.8323329.12515.15178752...   \n",
       "2      1.2.276.0.7230010.3.1.4.8323329.4904.151787518...   \n",
       "3      1.2.276.0.7230010.3.1.4.8323329.32579.15178751...   \n",
       "4      1.2.276.0.7230010.3.1.4.8323329.32579.15178751...   \n",
       "...                                                  ...   \n",
       "11577  1.2.276.0.7230010.3.1.4.8323329.4461.151787518...   \n",
       "11578  1.2.276.0.7230010.3.1.4.8323329.4461.151787518...   \n",
       "11579  1.2.276.0.7230010.3.1.4.8323329.32730.15178751...   \n",
       "11580  1.2.276.0.7230010.3.1.4.8323329.13252.15178752...   \n",
       "11581  1.2.276.0.7230010.3.1.4.8323329.12050.15178752...   \n",
       "\n",
       "                                           EncodedPixels  \n",
       "0                                                     -1  \n",
       "1                                                     -1  \n",
       "2       175349 7 1013 12 1009 17 1005 19 1003 20 1002...  \n",
       "3       407576 2 1021 7 1015 10 1013 12 1011 14 1008 ...  \n",
       "4       252069 1 1021 3 1020 4 1018 5 1018 6 1016 7 1...  \n",
       "...                                                  ...  \n",
       "11577   592067 6 1016 10 1012 14 1007 18 1004 20 1003...  \n",
       "11578   610576 3 1001 38 981 53 966 63 956 73 947 87 ...  \n",
       "11579                                                 -1  \n",
       "11580                                                 -1  \n",
       "11581                                                 -1  \n",
       "\n",
       "[11582 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_rle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 1024\n",
    "image_width = 1024\n",
    "channel_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader the level 1 of folder hierarchy\n",
    "level1_folder = glob.glob(f\"{train_data_path}/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load level 2 folder hierarchy\n",
    "level2_folder = []\n",
    "for idx, path in enumerate(level1_folder):\n",
    "    level2_folder.append(glob.glob(f\"{path}/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dicom file path at 3 level hierarchy\n",
    "dicom_file_path = []\n",
    "for i in range(len(level2_folder)):\n",
    "    dicom_file_path.append(glob.glob(f\"{level2_folder[i][0]}/*.dcm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dcm_img = []\n",
    "train_img_id = []\n",
    "for i in range(len(dicom_file_path)):\n",
    "    ds = pydicom.dcmread(dicom_file_path[i][0])\n",
    "    train_dcm_img.append(np.expand_dims(ds.pixel_array, axis=2))\n",
    "    train_img_id.append(ds.file_meta[(2, 3)].value) # consider ds as a large dictionary['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id = '1.2.276.0.7230010.3.1.4.8323329.10060.1517875221.792744'\n",
    "temp = train_rle.loc[train_rle['ImageId'] == id][' EncodedPixels'] # find the crossponding rle encoding\n",
    "display(temp.values.shape[0])\n",
    "# masks = []\n",
    "\n",
    "# for i in temp:\n",
    "#     masks.append(np.expand_dims(rle2mask(i, image_height, image_width).T, axis=2)) # convert rle to Pneumothorax image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exception, 1 x-ray corresponds to multiple masks, means multiple area has pneumonia!!!!!\n",
    "# for i in range(len(masks)):\n",
    "    # plt.figure(figsize=(5,5))\n",
    "    # plt.imshow(masks[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck\n",
      "65\n",
      "1.2.276.0.7230010.3.1.4.8323329.10060.1517875221.792744\n"
     ]
    }
   ],
   "source": [
    "# this processing has some flaw with the multiple area\n",
    "for idx, id in enumerate(train_img_id):\n",
    "    mask = train_rle.loc[train_rle['ImageId'] == id][' EncodedPixels'] # find the crossponding rle encoding\n",
    "    if mask.shape[0] == 0:\n",
    "        continue\n",
    "    mask = mask.values\n",
    "\n",
    "    if mask.shape[0] == 1:\n",
    "        if(mask != ' -1'):\n",
    "            train_masks.append(np.expand_dims(rle2mask(mask, image_height, image_width).T, axis=2)) # convert rle to Pneumothorax image\n",
    "        else:\n",
    "            train_masks.append(np.zeros((1024, 1024, 1))) # empty image\n",
    "    else:\n",
    "        temp = []\n",
    "        if(mask != ' -1'):\n",
    "            temp.append(np.expand_dims(rle2mask(mask, image_height, image_width).T, axis=2)) # convert rle to Pneumothorax image\n",
    "        else:\n",
    "            temp.append(np.zeros((1024, 1024, 1))) # empty image\n",
    "        train_masks.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(train_dcm_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(img.shape)\n",
    "temp = np.expand_dims(img, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10712, 1, 1024, 1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-f79c4c5673b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicom_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicom_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "data = np.zeros((len(dicom_file_path), image_height, image_width, channel_num), dtype=np.uint8)\n",
    "label = np.zeros((len(dicom_file_path), image_height, image_width, 1), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "    print(str(info)+\"\\n\")\n",
    "    \n",
    "\n",
    "def train_network(net, dl_train, dl_val, epochs, batch_size):\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss() # still need to determine the loss function\n",
    "    optimizer= torch.optim.Adam(net.parameters(),lr = 0.01)   \n",
    "\n",
    "    metrics_dict = {\"acc\":Accuracy()}\n",
    "\n",
    "    ckpt_path='checkpoint.pt'\n",
    "\n",
    "    #early_stopping相关设置\n",
    "    monitor=\"val_acc\"\n",
    "    patience=5\n",
    "    mode=\"max\"\n",
    "\n",
    "    history = {}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        printlog(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
    "\n",
    "        # 1，train -------------------------------------------------  \n",
    "        net.train()\n",
    "        \n",
    "        total_loss,step = 0,0\n",
    "        \n",
    "        loop = tqdm(enumerate(dl_train), total =len(dl_train))\n",
    "        train_metrics_dict = deepcopy(metrics_dict) \n",
    "        \n",
    "        for i, batch in loop: \n",
    "            \n",
    "            features,labels = batch\n",
    "            #forward\n",
    "            preds = net(features)\n",
    "            loss = loss_fn(preds,labels)\n",
    "            \n",
    "            #backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            #metrics\n",
    "            step_metrics = {\"train_\"+name:metric_fn(preds, labels).item() \n",
    "                            for name,metric_fn in train_metrics_dict.items()}\n",
    "            \n",
    "            step_log = dict({\"train_loss\":loss.item()},**step_metrics)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            step+=1\n",
    "            if i!=len(dl_train)-1:\n",
    "                loop.set_postfix(**step_log)\n",
    "            else:\n",
    "                epoch_loss = total_loss/step\n",
    "                epoch_metrics = {\"train_\"+name:metric_fn.compute().item() \n",
    "                                for name,metric_fn in train_metrics_dict.items()}\n",
    "                epoch_log = dict({\"train_loss\":epoch_loss},**epoch_metrics)\n",
    "                loop.set_postfix(**epoch_log)\n",
    "\n",
    "                for name,metric_fn in train_metrics_dict.items():\n",
    "                    metric_fn.reset()\n",
    "                    \n",
    "        for name, metric in epoch_log.items():\n",
    "            history[name] = history.get(name, []) + [metric]\n",
    "            \n",
    "\n",
    "        # 2，validate -------------------------------------------------\n",
    "        net.eval()\n",
    "        \n",
    "        total_loss,step = 0,0\n",
    "        loop = tqdm(enumerate(dl_val), total =len(dl_val))\n",
    "        \n",
    "        val_metrics_dict = deepcopy(metrics_dict) \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch in loop: \n",
    "\n",
    "                features,labels = batch\n",
    "                \n",
    "                #forward\n",
    "                preds = net(features)\n",
    "                loss = loss_fn(preds,labels)\n",
    "\n",
    "                #metrics\n",
    "                step_metrics = {\"val_\"+name:metric_fn(preds, labels).item() \n",
    "                                for name,metric_fn in val_metrics_dict.items()}\n",
    "\n",
    "                step_log = dict({\"val_loss\":loss.item()},**step_metrics)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                step+=1\n",
    "                if i!=len(dl_val)-1:\n",
    "                    loop.set_postfix(**step_log)\n",
    "                else:\n",
    "                    epoch_loss = (total_loss/step)\n",
    "                    epoch_metrics = {\"val_\"+name:metric_fn.compute().item() \n",
    "                                    for name,metric_fn in val_metrics_dict.items()}\n",
    "                    epoch_log = dict({\"val_loss\":epoch_loss},**epoch_metrics)\n",
    "                    loop.set_postfix(**epoch_log)\n",
    "\n",
    "                    for name,metric_fn in val_metrics_dict.items():\n",
    "                        metric_fn.reset()\n",
    "                        \n",
    "        epoch_log[\"epoch\"] = epoch           \n",
    "        for name, metric in epoch_log.items():\n",
    "            history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "        # 3，early-stopping -------------------------------------------------\n",
    "        arr_scores = history[monitor]\n",
    "        best_score_idx = np.argmax(arr_scores) if mode==\"max\" else np.argmin(arr_scores)\n",
    "        if best_score_idx==len(arr_scores)-1:\n",
    "            torch.save(net.state_dict(),ckpt_path)\n",
    "            print(\"<<<<<< reach best {0} : {1} >>>>>>\".format(monitor,\n",
    "                arr_scores[best_score_idx]),file=sys.stderr)\n",
    "        if len(arr_scores)-best_score_idx>patience:\n",
    "            print(\"<<<<<< {} without improvement in {} epoch, early stopping >>>>>>\".format(\n",
    "                monitor,patience),file=sys.stderr)\n",
    "            break \n",
    "        net.load_state_dict(torch.load(ckpt_path))\n",
    "        \n",
    "    dfhistory = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5d6c2be6262c78dadbf075e7ed7eef3fc1090c1044fdcff36d11e01ba01e819"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
